<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration</title>
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration" />
	<meta property="og:description" content="Reliable multimodal sensor fusion algorithms require accurate spatiotemporal calibration. 
		Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. 
		Nevertheless, such methods are inherently slow to train given the high computational overhead caused by the large number of sampled points required for volume rendering. 
		With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new rendering approach to achieve faster multi-sensor calibration. 
		We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, 
		robust, and with a substantial speed-up compared to methods relying on implicit neural representations. 
		We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset." />
	<meta name="google-site-verification" content="oerUbOFOxhAdGWX30cdj8RT69T7_30mlD8OZdDf02-g" />
	<meta name="description" content="Reliable multimodal sensor fusion algorithms require accurate spatiotemporal calibration. 
		Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. 
		Nevertheless, such methods are inherently slow to train given the high computational overhead caused by the large number of sampled points required for volume rendering. 
		With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new rendering approach to achieve faster multi-sensor calibration. 
		We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, 
		robust, and with a substantial speed-up compared to methods relying on implicit neural representations. 
		We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset.">
 	<meta name="keywords" content="calibration, multimodal, spatiotemporal, lidar, camera, nerf, neural radiance field, targetless, automatic, 3DGS-Calib, 3D, Gaussian, Splatting">	

</head>
	<script async src=""></script>
<body>
	<br>
	<center>
		<p>
			<span style="font-size:36px">3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration</span><br>
		</p>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://www.linkedin.com/in/quentin-herau-38378b140/">Quentin Herau</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?hl=en&user=0jLPiLYAAAAJ">Moussab Bennehar</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.com/citations?user=fMmoHX0AAAAJ&hl">Arthur Moreau</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?user=S3zYmOYAAAAJ&hl">Nathan Piasco</a></span>
						</center>
					</td>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?hl=en&user=bRgp2lUAAAAJ">Luis Rold&atilde;o</a></span>
						</center>
					</td>

			
				</tr>
			</table>
			<table align=center width=1000px>

				<tr>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://www.linkedin.com/in/dzmitry-tsishkou-9b287724/">Dzmitry Tsishkou</a></span>
						</center>
					</td>

					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?user=PB2OanoAAAAJ&hl">Cyrille Migniot</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.com/citations?user=5dPw73sAAAAJ&hl">Pascal Vasseur</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://sites.google.com/view/cedricdemonceaux/home">C&eacute;dric Demonceaux</a></span>
						</center>

			
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2403.11577'>[Paper]</a></span>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				<p align="justify">Reliable multimodal sensor fusion algorithms require accurate spatiotemporal calibration. 
					Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. 
					Nevertheless, such methods are inherently slow to train given the high computational overhead caused by the large number of sampled points required for volume rendering. 
					With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, 
					we propose to leverage this new rendering approach to achieve faster multi-sensor calibration. We introduce 3DGS-Calib, 
					a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, 
					robust, and with a substantial speed-up compared to methods relying on implicit neural representations. 
					We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset.</p>
			</td>
		</tr>
	</table>
	<br>


	<hr>

	<center><h1>Framework</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<p align="justify"><b>Pipeline of 3DGS-Calib: </b>
					The Gaussiansâ€™ positions are given as input to the neural network which predicts their parameters. In parallel, the
					calibration parameters provide the input pose that transforms the Gaussians from the world frame to the image frame. Then, the 3D Gaussians are splatted
					using their predicted parameters to generate the rendered image. This image is compared to its ground-truth (GT) counterpart to compute the photometric
					loss. Finally, the gradients are backpropagated to the neural network and the calibration parameters.</p>
				</td>
			</tr>
		</center>
	</table>
	<br>

	<hr>
	<center><h1>Supplementary video</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/ne4lLaBr4kk?si=h1lwncYXnd3XRiiA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>
	
	<hr>
	<table align=center width=450px>
		<center><h1>Cite</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Q. Herau, M. Bennehar, A. Moreau, N. Piasco,  L. Rold&atilde;o, D. Tsishkou, C. Migniot, P. Vasseur, C. Demonceaux.<br>
				<b>MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal calibration.</b><br>
				<!-- In Conference, 20XX.<br> -->
				(hosted on <a href="https://arxiv.org/abs/2403.11577">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

<br>
</body>
</html>

