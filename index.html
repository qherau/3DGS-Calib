<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal calibration</title>
	<!--<meta property="og:image" content="Path to my teaser.png"/>-->
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal calibration" />
	<meta property="og:description" content="With the recent advances in autonomous driving and the decreasing cost of LiDARs, the use of multimodal sensor systems is on the rise. 
		However, in order to make use of the information provided by a variety of complimentary sensors, it is necessary to accurately calibrate them. 
		We take advantage of recent advances in computer graphics and implicit volumetric scene representation to tackle the problem of multi-sensor spatial and temporal calibration. 
		Thanks to a new formulation of the Neural Radiance Field (NeRF) optimization, we are able to jointly optimize calibration parameters along with scene representation 
		based on radiometric and geometric measurements. Our method enables accurate and robust calibration from data captured in uncontrolled and unstructured urban environments, 
		making our solution more scalable than existing calibration solutions. 
		We demonstrate the accuracy and robustness of our method in urban scenes typically encountered in autonomous driving scenarios." />
	<meta name="google-site-verification" content="oerUbOFOxhAdGWX30cdj8RT69T7_30mlD8OZdDf02-g" />
	<meta name="description" content="With the recent advances in autonomous driving and the decreasing cost of LiDARs, the use of multimodal sensor systems is on the rise. 
		However, in order to make use of the information provided by a variety of complimentary sensors, it is necessary to accurately calibrate them. 
		We take advantage of recent advances in computer graphics and implicit volumetric scene representation to tackle the problem of multi-sensor spatial and temporal calibration. 
		Thanks to a new formulation of the Neural Radiance Field (NeRF) optimization, we are able to jointly optimize calibration parameters along with scene representation 
		based on radiometric and geometric measurements. Our method enables accurate and robust calibration from data captured in uncontrolled and unstructured urban environments, 
		making our solution more scalable than existing calibration solutions. 
		We demonstrate the accuracy and robustness of our method in urban scenes typically encountered in autonomous driving scenarios.">
 	<meta name="keywords" content="calibration, multimodal, spatiotemporal, lidar, camera, nerf, neural radiance field, targetless, automatic, iros, moisst">	

</head>
	<script async src=""></script>
<body>
	<br>
	<center>
		<p>
			<span style="font-size:36px">MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal calibration</span><br>
			<span style="font-size:32px">IROS 2023</span>
		</p>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://www.linkedin.com/in/quentin-herau-38378b140/">Quentin Herau</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?user=S3zYmOYAAAAJ&hl">Nathan Piasco</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?hl=en&user=0jLPiLYAAAAJ">Moussab Bennehar</a></span>
						</center>
					</td>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?hl=en&user=bRgp2lUAAAAJ">Luis Rold&atilde;o</a></span>
						</center>
					</td>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://www.linkedin.com/in/dzmitry-tsishkou-9b287724/">Dzmitry Tsishkou</a></span>
						</center>
					</td>
			
				</tr>
			</table>
			<table align=center width=1000px>

				<tr>

					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?user=PB2OanoAAAAJ&hl">Cyrille Migniot</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.com/citations?user=5dPw73sAAAAJ&hl">Pascal Vasseur</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://sites.google.com/view/cedricdemonceaux/home">C&eacute;dric Demonceaux</a></span>
						</center>

			
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2303.03056'>[Paper]</a></span>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<p align="justify"><b>Effect of calibration on novel view synthesis:</b> 
					training positions in
					<span style="color: #ff0000">red</span>, ground truth positions in 
					<span style="color: #00ff00">green</span>, 
					reference position in <span style="color: #0000ff">blue</span>. The RGB
					images (top) and depth maps (middle) are rendered from an implicit neural
					3D scene trained from non-calibrated (left) and calibrated with MOISST
					(right) sensors.</p>
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				<p align="justify">With the recent advances in autonomous driving and the decreasing cost of LiDARs, 
					the use of multimodal sensor systems is on the rise. 
					However, in order to make use of the information provided by a variety of complimentary sensors, 
					it is necessary to accurately calibrate them. We take advantage of recent advances in computer graphics 
					and implicit volumetric scene representation to tackle the problem of multi-sensor spatial and temporal calibration. 
					Thanks to a new formulation of the Neural Radiance Field (NeRF) optimization, 
					we are able to jointly optimize calibration parameters along with scene representation based on radiometric and geometric measurements. 
					Our method enables accurate and robust calibration from data captured in uncontrolled and unstructured urban environments, 
					making our solution more scalable than existing calibration solutions. 
					We demonstrate the accuracy and robustness of our method in urban scenes typically encountered in autonomous driving scenarios.</p>
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Supplementary video</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/RY1QkHnaoA0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>
	<hr>

	<center><h1>Framework</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<p align="justify"><b>Overview of MOISST optimization framework: </b>
					First, the model is initialized with rays generated using rough spatial and temporal calibration
					priors in addition to the reference frame trajectory. After each optimization step, the rays are regenerated and fed to the NeRF model. We then render
					RGB images and depth maps which are used along the ground truth ones to compute the losses and propagate the gradients. Gradient descent algorithm
					is finally used to optimize both NeRF and calibration parameters.</p>
				</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Cite</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Q. Herau, N. Piasco, M. Bennehar, L. Rold&atilde;o, D. Tsishkou, C. Migniot, P. Vasseur, C. Demonceaux.<br>
				<b>MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal calibration.</b><br>
				<!-- In Conference, 20XX.<br> -->
				(hosted on <a href="https://arxiv.org/abs/2303.03056">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

<br>
</body>
</html>

